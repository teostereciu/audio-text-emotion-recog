{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Audio preprocessing"
      ],
      "metadata": {
        "id": "U9Ln_sGmBQlx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0syuauj__3ii"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def uniform_length(str, max_len=1000):\n",
        "    ls = list(eval(str))\n",
        "    if len(ls) < max_len:\n",
        "        to_append = [0]*(max_len - len(ls))\n",
        "        ls += to_append\n",
        "    ls = ls[:max_len]\n",
        "    return repr(ls)\n",
        "\n",
        "def add_feature(srs, split, features):\n",
        "    for feature in srs.index:\n",
        "        str = srs[feature]\n",
        "        ls = list(eval(str))\n",
        "        if feature in features[split].keys():\n",
        "            features[split][feature] += [ls]\n",
        "        else:\n",
        "            features[split][feature] = [ls]\n",
        "\n",
        "    return features\n",
        "\n",
        "def get_audio_features(df):\n",
        "\n",
        "    ignore = ['filename', 'id', 'conversation_line', 'emotion', 'speaker', 'split']\n",
        "    X_train_audio = np.array(df[df['split'] == 'train'].drop(columns=ignore).values.tolist())\n",
        "    X_val_audio = np.array(df[df['split'] == 'val'].drop(columns=ignore).values.tolist())\n",
        "    X_test_audio = np.array(df[df['split'] == 'test'].drop(columns=ignore).values.tolist())\n",
        "\n",
        "    return X_train_audio, X_val_audio, X_test_audio\n",
        "\n",
        "def do_oversample(X, labels, type='SMOTE'):\n",
        "    strategy = {'disgust':500, 'fear':500}\n",
        "    if type=='SMOTE':\n",
        "        os = SMOTE(sampling_strategy=strategy)\n",
        "    else:\n",
        "        os = RandomOverSampler(sampling_strategy=strategy)\n",
        "    X, labels = os.fit_resample(X, labels)\n",
        "    return X, labels\n",
        "\n",
        "def do_scale(X, scaler=None):\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler()\n",
        "        scaled_X = scaler.fit_transform(X)\n",
        "        return scaled_X, scaler\n",
        "    else:\n",
        "        return scaler.transform(X)\n",
        "\n",
        "def do_pca(X, pca=None, n_comp=500):\n",
        "    if pca is None:\n",
        "        pca = PCA(n_comp)\n",
        "        pca_X = pca.fit_transform(X)\n",
        "        return pca_X, pca\n",
        "    else:\n",
        "        return pca.transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text preprocessing"
      ],
      "metadata": {
        "id": "-8oKCEkkBU-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, TFBertModel\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def get_text_features(df):\n",
        "    train_df = df[df['split'] == 'train']\n",
        "    val_df = df[df['split'] == 'val']\n",
        "    test_df = df[df['split'] == 'test']\n",
        "\n",
        "    train_text = train_df['conversation_line'].to_list()\n",
        "    val_text = val_df['conversation_line'].to_list()\n",
        "    test_text = test_df['conversation_line'].to_list()\n",
        "\n",
        "    return train_text, val_text, test_text\n",
        "    #return np.array(train_text),np.array(val_text), np.array(test_text)\n",
        "\n",
        "\n",
        "def bert_encode_text(X_train, X_val, X_test):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", use_fast=True)\n",
        "    model = TFBertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "\n",
        "    print('Tokenizer is fast?', tokenizer.is_fast)\n",
        "\n",
        "    def tkn(text):\n",
        "        print('*')\n",
        "        return tokenizer(text, return_tensors=\"tf\", padding='max_length', truncation=True, max_length=30)\n",
        "\n",
        "    def encode(X):\n",
        "        batch_size = 300\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(X).batch(batch_size)\n",
        "        pooled_outputs = []\n",
        "        i = 0\n",
        "        for batch in dataset:\n",
        "            print('batch', i)\n",
        "            #print(batch.numpy().tolist())\n",
        "            texts = [str(text.numpy(), 'utf-8') for text in batch]\n",
        "            inputs = tkn(texts)\n",
        "            outputs = model(inputs)\n",
        "            pooled_output = outputs['pooler_output']\n",
        "            pooled_outputs.append(pooled_output)\n",
        "            i += 1\n",
        "        return tf.concat(pooled_outputs, axis=0)\n",
        "\n",
        "    return encode(X_train), encode(X_val), encode(X_test)"
      ],
      "metadata": {
        "id": "WApOU72JBW3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make dataset"
      ],
      "metadata": {
        "id": "tk5IY5umCNQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import opensmile\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def get_data_from_json(json_convo, convo_idx):\n",
        "    \"\"\" Extracts text, emotion, file name, and speaker labels from\n",
        "        a json conversation.\n",
        "    \"\"\"\n",
        "\n",
        "    lines = []\n",
        "    emotions = []\n",
        "    filenames = []\n",
        "    speakers = []\n",
        "    ids = []\n",
        "    for line_idx in range(len(json_convo)):\n",
        "        lines += [json_convo[line_idx]['text']]\n",
        "        emotions += [json_convo[line_idx]['emotion']]\n",
        "        filenames += [json_convo[line_idx]['video_name'][:-4]]\n",
        "        speakers += [json_convo[line_idx]['speaker']]\n",
        "        # label convo lines so we can still track which conversations they came from\n",
        "        # and what position they have within the conversation\n",
        "        id = str(convo_idx) + str(line_idx)\n",
        "        ids += [id]\n",
        "    return lines, emotions, filenames, speakers, ids\n",
        "\n",
        "def extract_audio_features_from_wav(filename, split, smile):\n",
        "    \"\"\" Extracts smile audio features from a wav file. \"\"\"\n",
        "    filename += '.wav'\n",
        "    filepath = interim_data_dir / split / filename\n",
        "\n",
        "    if not os.path.isfile(filepath):\n",
        "        return None # split is unknown so we try everywhere, if file not here skip\n",
        "\n",
        "    result_df = smile.process_file(filepath)\n",
        "    rename_split = {'train_wav': 'train', 'dev_wav': 'val', 'test_wav': 'test'}\n",
        "    result_df['split'] = rename_split[split]\n",
        "\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(filename)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\" Runs data processing scripts to turn raw data from (../raw) into\n",
        "        cleaned data ready to be analyzed (saved in ../processed).\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info('building dataset')\n",
        "    logger.info('loading json data')\n",
        "    text_data = json.loads(json_data_path.read_text())\n",
        "\n",
        "    all_lines = []\n",
        "    all_emotions = []\n",
        "    all_filenames = []\n",
        "    all_speakers = []\n",
        "    all_ids = []\n",
        "\n",
        "    for convo_idx in range(len(text_data)):\n",
        "        lines, emotions, filenames, speakers, ids = get_data_from_json(text_data[convo_idx]['conversation'], convo_idx)\n",
        "        all_lines += lines\n",
        "        all_emotions += emotions\n",
        "        all_filenames += filenames\n",
        "        all_speakers += speakers\n",
        "        all_ids += ids\n",
        "\n",
        "    data_dict = {'filename': all_filenames, 'id': all_ids, 'conversation_line' : all_lines, 'emotion': all_emotions, 'speaker': all_speakers}\n",
        "    df = pd.DataFrame.from_dict(data_dict).astype({'id':str})\n",
        "\n",
        "    logger.info('done loading json data')\n",
        "    # rn assume wav have been extracted from mp4 to ../interim\n",
        "    logger.info(f'extracting smile features from wav files in {interim_data_dir}')\n",
        "\n",
        "    smile = opensmile.Smile(\n",
        "        feature_set=opensmile.FeatureSet.emobase,\n",
        "        feature_level=opensmile.FeatureLevel.Functionals,\n",
        "    )\n",
        "\n",
        "    all_features = []\n",
        "\n",
        "    for fname in data_dict['filename']:\n",
        "        features = extract_audio_features_from_wav(fname, 'train_wav', smile)\n",
        "        if features is None:\n",
        "            features = extract_audio_features_from_wav(fname, 'dev_wav', smile)\n",
        "        if features is None:\n",
        "            features = extract_audio_features_from_wav(fname, 'test_wav', smile)\n",
        "\n",
        "        all_features += [features]\n",
        "\n",
        "    audio_features_df = pd.concat(all_features, axis=0)\n",
        "    audio_features_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    logger.info('done extracting functional smile features from wav')\n",
        "    logger.info('creating final dataset')\n",
        "\n",
        "    concatenated_df = pd.concat([df, audio_features_df], axis=1)\n",
        "    save_path = processed_data_dir / 'processed_func_data.csv'\n",
        "    concatenated_df.to_csv(save_path, index=False)\n",
        "\n",
        "    logger.info(f'done creating final dataset. saved to: {save_path}')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "    logging.basicConfig(level=logging.INFO, format=log_fmt)\n",
        "\n",
        "    project_dir = Path(__file__).resolve().parents[2]\n",
        "    raw_data_dir = project_dir / 'data' / 'raw'\n",
        "    interim_data_dir = project_dir / 'data' / 'interim'\n",
        "    processed_data_dir = project_dir / 'data' / 'processed'\n",
        "    json_data_path = raw_data_dir / 'Subtask_2_train.json'\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "id": "bQTgqSk6CQUe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}